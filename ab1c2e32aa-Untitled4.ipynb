{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled4.ipynb","provenance":[],"authorship_tag":"ABX9TyNawl/oYmVWvP89JmE9Tk1y"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"n8gktJLNIAjK","colab_type":"text"},"source":["First Import important libraries"]},{"cell_type":"code","metadata":{"id":"U-YqLO6oH_DV","colab_type":"code","colab":{}},"source":["import re # for regular expressions\n","import pandas as pd \n","pd.set_option(\"display.max_colwidth\", 200)\n","import numpy as np \n","import matplotlib.pyplot as plt \n","import seaborn as sns\n","import string\n","import nltk # for text manipulation\n","import warnings \n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fet4cUvfIMAo","colab_type":"text"},"source":["Read Files"]},{"cell_type":"code","metadata":{"id":"60zzHABLIQ_0","colab_type":"code","colab":{}},"source":["train  = pd.read_csv('sample_data/train.csv')\n","test = pd.read_csv('sample_data/test.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GvipACkpIWzj","colab_type":"code","colab":{}},"source":["train[train['sentiment_class'] == 0].head(10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2fgETojVIdTC","colab_type":"text"},"source":["Analyze"]},{"cell_type":"code","metadata":{"id":"N8w2UXcTIfnc","colab_type":"code","colab":{}},"source":["length_train = train['original_text'].str.len()\n","length_test = test['original_text'].str.len()\n","\n","plt.hist(length_train, bins=20, label=\"train_tweets\")\n","plt.hist(length_test, bins=20, label=\"test_tweets\")\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pQEBBchoIkkW","colab_type":"code","colab":{}},"source":["combi = train.append(test, ignore_index=True)\n","combi.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WigIKVhSInoE","colab_type":"text"},"source":["DATA CLEANING"]},{"cell_type":"code","metadata":{"id":"QEJtiK_2IpJQ","colab_type":"code","colab":{}},"source":["def remove_pattern(input_txt, pattern):\n","    r = re.findall(pattern, input_txt)\n","    for i in r:\n","        input_txt = re.sub(i, '', input_txt)\n","        \n","    return input_txt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cekozOuIIqD8","colab_type":"code","colab":{}},"source":["combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['original_text'], \"[@,xxx][\\w]*\") \n","combi.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zCpyGReSIzL5","colab_type":"text"},"source":["Remove Small words"]},{"cell_type":"code","metadata":{"id":"x17VGRAAI1De","colab_type":"code","colab":{}},"source":["combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"60WxoguHI2Uh","colab_type":"text"},"source":["Tokenization"]},{"cell_type":"code","metadata":{"id":"iR9Vc-DAI67S","colab_type":"code","colab":{}},"source":["tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Q6xmsh3I-Q5","colab_type":"text"},"source":["Stemming for root words"]},{"cell_type":"code","metadata":{"id":"OFrWLZh_JAXq","colab_type":"code","colab":{}},"source":["from nltk.stem.porter import *\n","stemmer = PorterStemmer()\n","\n","tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QqnaueW-JEXP","colab_type":"code","colab":{}},"source":["\n","for i in range(len(tokenized_tweet)):\n","    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n","    \n","combi['tidy_tweet'] = tokenized_tweet"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c_qtdMWpJIRx","colab_type":"text"},"source":["Hashtag Extraction"]},{"cell_type":"code","metadata":{"id":"kS4e7fJIJKLq","colab_type":"code","colab":{}},"source":["def hashtag_extract(x):\n","    hashtags = []\n","    # Loop over the words in the tweet\n","    for i in x:\n","        ht = re.findall(r\"#(\\w+)\", i)\n","        hashtags.append(ht)\n","\n","    return hashtags"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UQELFsUIJOV-","colab_type":"code","colab":{}},"source":["HT_neutral = hashtag_extract(combi['tidy_tweet'][combi['sentiment_class'] == 0])\n","\n","HT_positive = hashtag_extract(combi['tidy_tweet'][combi['sentiment_class'] == 1])\n","\n","\n","HT_negative = hashtag_extract(combi['tidy_tweet'][combi['sentiment_class'] == -1])\n","\n","\n","\n","# unnesting list\n","HT_neutral = sum(HT_neutral,[])\n","HT_positive = sum(HT_positive,[])\n","HT_negative = sum(HT_negative,[])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EZWOd2GOJTmA","colab_type":"code","colab":{}},"source":["\n","a = nltk.FreqDist(HT_neutral)\n","d = pd.DataFrame({'Hashtag': list(a.keys()),\n","                  'Count': list(a.values())})\n","\n","# selecting top 20 most frequent hashtags     \n","d = d.nlargest(columns=\"Count\", n = 20) \n","plt.figure(figsize=(16,5))\n","ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n","ax.set(ylabel = 'Count')\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"knn7EIXAJXY1","colab_type":"code","colab":{}},"source":["\n","b = nltk.FreqDist(HT_negative)\n","e = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})\n","\n","# selecting top 20 most frequent hashtags\n","e = e.nlargest(columns=\"Count\", n = 20)   \n","plt.figure(figsize=(16,5))\n","ax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IeA4sLt_Jb_f","colab_type":"text"},"source":["importing genism and feature extraction libraries"]},{"cell_type":"code","metadata":{"id":"JnW3ETVkJetf","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","import gensim"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d0QpO7N6JlIX","colab_type":"text"},"source":["Generate Bag of words"]},{"cell_type":"code","metadata":{"id":"033U5BnzJnjd","colab_type":"code","colab":{}},"source":["\n","bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n","bow = bow_vectorizer.fit_transform(combi['tidy_tweet'])\n","bow.shape\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yU8vK9-xJqwk","colab_type":"code","colab":{}},"source":["tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n","tfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet'])\n","tfidf.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rO3lytYkJvoq","colab_type":"code","colab":{}},"source":["tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing\n","\n","model_w2v = gensim.models.Word2Vec(\n","            tokenized_tweet,\n","            size=200, # desired no. of features/independent variables \n","            window=5, # context window size\n","            min_count=2,\n","            sg = 1, # 1 for skip-gram model\n","            hs = 0,\n","            negative = 10, # for negative sampling\n","            workers= 2, # no.of cores\n","            seed = 34)\n","\n","model_w2v.train(tokenized_tweet, total_examples= len(combi['tidy_tweet']), epochs=20)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_vQaeQZTJ0L6","colab_type":"text"},"source":["Creating W2v"]},{"cell_type":"code","metadata":{"id":"jOd2eNmsJ3Y_","colab_type":"code","colab":{}},"source":["def word_vector(tokens, size):\n","    vec = np.zeros(size).reshape((1, size))\n","    count = 0.\n","    for word in tokens:\n","        try:\n","            vec += model_w2v[word].reshape((1, size))\n","            count += 1.\n","        except KeyError: # handling the case where the token is not in vocabulary\n","                         \n","            continue\n","    if count != 0:\n","        vec /= count\n","    return vec"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jEdp9LlDJ6jm","colab_type":"code","colab":{}},"source":["\n","wordvec_arrays = np.zeros((len(tokenized_tweet), 200))\n","\n","for i in range(len(tokenized_tweet)):\n","    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)\n","    \n","wordvec_df = pd.DataFrame(wordvec_arrays)\n","wordvec_df.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HtgrkQzAJ-SZ","colab_type":"text"},"source":["For labels"]},{"cell_type":"code","metadata":{"id":"FeQqrd0pJ_fU","colab_type":"code","colab":{}},"source":["from tqdm import tqdm\n","tqdm.pandas(desc=\"progress-bar\")\n","from gensim.models.doc2vec import LabeledSentence"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0qwY7ycXKB3V","colab_type":"code","colab":{}},"source":["def add_label(twt):\n","    output = []\n","    for i, s in zip(twt.index, twt):\n","        output.append(LabeledSentence(s, [\"tweet_\" + str(i)]))\n","    return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1lqWHj3bKEM0","colab_type":"code","colab":{}},"source":["labeled_tweets = add_label(tokenized_tweet)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kd98LqnkKLZl","colab_type":"code","colab":{}},"source":["model_d2v = gensim.models.Doc2Vec(dm=1, # dm = 1 for ‘distributed memory’ model \n","                                  dm_mean=1, # dm = 1 for using mean of the context word vectors\n","                                  size=200, # no. of desired features\n","                                  window=5, # width of the context window\n","                                  negative=7, # if > 0 then negative sampling will be used\n","                                  min_count=5, # Ignores all words with total frequency lower than 2.\n","                                  workers=3, # no. of cores\n","                                  alpha=0.1, # learning rate\n","                                  seed = 23)\n","\n","model_d2v.build_vocab([i for i in tqdm(labeled_tweets)])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uIOoPeM9KQA1","colab_type":"text"},"source":["Training"]},{"cell_type":"code","metadata":{"id":"AHZKEsKOKRJG","colab_type":"code","colab":{}},"source":["model_d2v.train(labeled_tweets, total_examples= len(combi['tidy_tweet']), epochs=25)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vmr1hoyBKTV6","colab_type":"code","colab":{}},"source":["docvec_arrays = np.zeros((len(tokenized_tweet), 200))\n","\n","for i in range(len(combi)):\n","    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,200))\n","    \n","docvec_df = pd.DataFrame(docvec_arrays)\n","docvec_df.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LWKYHFBmKWVR","colab_type":"text"},"source":["Apply Logistic Regression"]},{"cell_type":"code","metadata":{"id":"4tsNvFciKaQ4","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-XiG08VYKdes","colab_type":"text"},"source":["Splitting data"]},{"cell_type":"code","metadata":{"id":"ynPLBbm4KewD","colab_type":"code","colab":{}},"source":["train_bow = bow[:3235,:]\n","test_bow = bow[3235:,:]\n","\n","# splitting data into training and validation set\n","xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['sentiment_class'],  \n","                                                          random_state=42, \n","                                                          test_size=0.3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4kMt-EDUKhlG","colab_type":"code","colab":{}},"source":["lreg = LogisticRegression(max_iter=7600)\n","lreg.fit(xtrain_bow, ytrain) # training the model\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uO3KNK4rKk-N","colab_type":"text"},"source":["XG BOOST for inhance the model"]},{"cell_type":"code","metadata":{"id":"CHRg42YtKnTB","colab_type":"code","colab":{}},"source":["from xgboost import XGBClassifier\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qCpbM5vWKqa5","colab_type":"code","colab":{}},"source":["xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_bow, ytrain)\n","prediction = xgb_model.predict(xvalid_bow)\n","100*f1_score(yvalid, prediction,average='weighted')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pvF8DgIzK69K","colab_type":"code","colab":{}},"source":["test_pred = xgb_model.predict(test_bow)\n","test['sentiment_class'] = test_pred\n","submission = test[['id','sentiment_class']]\n","submission.to_csv('submission.csv', index=False)"],"execution_count":0,"outputs":[]}]}